Safe Multi-Agent Scenarios
As advanced AI systems proliferate, they will increasingly operate in environments populated by other AIs and by humans—with complex, often conflicting goals. This multi-agent context raises profound safety challenges and new opportunities for cooperation. Interactions between multiple agents—whether human, artificial, or hybrid—can produce emergent behaviors that are hard to predict, control, or align.

Without careful design, such systems could foster deception, collusion, power concentration, or societal fragmentation. Yet if properly guided, multi-agent systems could instead enable an explosion in technologies for mutually-beneficial cooperation, benefiting individual one-to-one interactions as well as society-wide collaboration on collective action problems.


In this area, we seek proposals that explore how to upgrade today’s cooperation infrastructure of norms, laws, rights, and institutions to ensure that humans and AI systems can interact safely and beneficially in multi-agent settings. We are particularly interested in prototypes that address new game-theoretic dynamics and principal-agent problems that will arise in interactions between AI agents and humans, mitigate risks of collusion and deception, and enhance mechanisms for trustworthy communication, negotiation, and coordination. We aim to de-risk the deployment of cooperative AI and create pathways where AI agents strengthen rather than undermine human-aligned cooperation.

Early demonstrations—such as AI systems that assist humans in negotiation or agents that autonomously identify and enforce mutually beneficial deals or detect and punish deception— could lay the foundation for a future where interactions among AIs and humans are predictably safe, transparent, and welfare-improving. We also welcome projects that lift collective intelligence at the group level, using AI to augment the processes through which groups form shared preferences, resolve conflicts, and coordinate action.

Building safe multi-agent systems is not just about designing good individual agents. It is about shaping the ecosystem of interactions, incentives, and norms that will govern how AIs and humans co-evolve together.

Specific Work We Would Like to Fund
AI for Preventing Collusion and Manipulation
Scalable solutions that demonstrably prevent collusion, manipulation, or exploitation in agent-mediated agreements.

Scalable methods for detecting and mitigating new forms of AI deception and collusion against humans enabled by their distinct ability to communicate, commit and problem-solve.
Scalable methods for detecting and mitigating indirectly harmful social dynamics like polarization, groupthink, or majority tyranny in AI-mediated group settings.
Pareto-Preferred Coordination Agents
Autonomous agents that can identify, negotiate, and enforce mutually beneficial arrangements between humans and other AI systems.

Mechanisms for credible commitment and agreement enforcement that enable trust in automated coordination.
Systems that lower transaction costs for beneficial coordination while preserving human autonomy and oversight.
AI-Enhanced Group Coordination
AI systems that enhance collective intelligence and enable more effective group coordination around shared preferences.

AI tools that improve information aggregation, preference elicitation, and consensus-building in group decision processes.
AI-native institutions that leverage the unique commitment properties of AI systems to build new forms of cooperation and governance among human or AI systems.
Our Priorities
Proposals should clearly demonstrate how the work will enhance safety in multi-agent AI environments, with particular attention to preventing harmful emergent dynamics when multiple AI systems interact with each other and humans.

We prioritize projects that:

Emphasize practical implementations that can be tested and refined in real-world multi-agent scenarios, rather than purely theoretical approaches.
Address power imbalances by designing systems that protect vulnerable participants and prevent unhealthy concentration of influence.
Incorporate robust safeguards against collusion, deception, and adversarial manipulation between agents.
Promote open source, transparent interaction protocols that make agent behaviors, incentives, and decision processes legible to humans and other systems.
Enable beneficial coordination while preserving individual autonomy and diversity of values across participating agents.
Previously Funded Work
Examples of past projects in this area include:

Socially Complex Multi-Agent Game Environments – Development of tools and training of agents in dynamic, socially rich multi-agent environments to enable both safety and capabilities research on cooperation, conflict, and emergent behavior.
Emergent Collusion Detection in Multi-Agent Reinforcement Learning – Creation of methods for detecting collusion and collaboration among large populations of AI agents based on limited action observations, advancing early-warning systems for multi-agent risks.
Active Inference Models for Multi-Agent Systems – Application of Active Inference theory to build bounded-rational agents with improved cooperative capabilities, reduced disempowerment behaviors, and more robust environment shaping.
Multipolar and Game-Theoretic Simulation Platform – Construction of a computational model to simulate multipolar and game-theoretic dynamics among AI systems, enabling rigorous testing of cooperation and conflict scenarios at scale.
Ph.D Research in Multi-Agent Safety at Oxford – Support for early-stage research careers focused on multi-agent system safety, helping to build a pipeline of expertise at leading academic institutions.
Grantees

Chandler Smith
Cooperative AI Foundation

An exploration into multi-agent security, steganography, and AI control

David Bloomin
MettaAi

Open-Ended Learning in Socially Complex Multi-Agent Environments

Jonas Emanuel Müller
Convergence Analysis

Exploring AI Scenarios via the Intelligence Rising Web Application

Keenan Pepper
AE Studio

Embedded Agency Playgrounds

Kola Ayonrinde
MATS

Active Inference Multi-Agent Interactions

Nora Ammann
PIBBSS

PIBBSS Fellowship 2024

Joel Pyykkö
Independent

Aintelope

Roland Pihlakas
Simplify (Macrotec LLC)

Aintelope

Toby D. Pilditch
University of Oxford

Cutting Through the Complexity of Multi-agent AI Scenarios

Leonardo Christov-Moore
Institute for Advanced Consciousness Studies

Homeostasis and Extinction in Multiagent Reinforcement Learning

MATS Research
ML Alignment & Theory Scholars (MATS) Program
References
Foundations of Cooperative AI | Conitzer & Oesterheld
Paretotopian Goal Alignment | Drexler
Reframing Superintelligence: Comprehensive AI Services as General Intelligence | Drexler
[arXiv:1804.04268] Incomplete Contracting and AI Alignment | Hadfield-Menell & Hadfield
[arXiv:2208.07006] Cooperative and Uncooperative Institution Designs: Surprises and Problems in Open-Source Game Theory | Critch, Dennis & Russell
[arXiv:2402.17270] Multi-Agent, Human-Agent and Beyond | Mu et al.
[arXiv:2402.07510] Secret Collusion Among AI Systems: Multi-Agent Deception via Steganography | Motwani et al.
[arXiv:2501.10114] Infrastructure for AI Agents | Chan et al.
[arXiv:2412.10270] Cultural Evolution in AI Agents | Drexler
[arXiv:2503.01769] Using Collective Dialogues and AI to Find Common Ground Between Israeli and Palestinian Peacebuilders | Konya et al.
Multi-Agent Safety Hackathon | Cooperative AI Foundation (Various contributors)
Multi-Agent Risks from Advanced AI | Cooperative AI Foundation
How to Mitigate AI-Driven Power Concentration | Future of Life Institute
AI Tools for Existential Security | Forethought Foundation
